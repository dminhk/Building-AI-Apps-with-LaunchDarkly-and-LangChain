{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72ea4020-d716-4119-8c19-7fde35c5faec",
   "metadata": {},
   "source": [
    "# Building AI Apps with LaunchDarkly and LangChain\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook example demonstrates how to run the \"Building AI Apps with LaunchDarkly and LangChain\" sample codes in Amazon Bedrock.\n",
    "\n",
    "Credits to the original writer who posted the excellent content.\n",
    "\n",
    "You can follow along with this notebook while referring to the original post.\n",
    "* https://docs.launchdarkly.com/guides/infrastructure/langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da435b14-eeae-4b05-8f46-bf1725d9dd20",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d931f434-043f-4c61-8a65-00ac7fe491d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU boto3 langchain langchain-aws launchdarkly-server-sdk python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c200fb-a6cc-44a1-a9a3-4eb4706446ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain\n",
      "Version: 0.1.16\n",
      "Summary: Building applications with LLMs through composability\n",
      "Home-page: https://github.com/langchain-ai/langchain\n",
      "Author: \n",
      "Author-email: \n",
      "License: MIT\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: aiohttp, async-timeout, dataclasses-json, jsonpatch, langchain-community, langchain-core, langchain-text-splitters, langsmith, numpy, pydantic, PyYAML, requests, SQLAlchemy, tenacity\n",
      "Required-by: jupyter_ai_magics, langchain-experimental, langserve, llama-index-llms-langchain\n",
      "---\n",
      "Name: launchdarkly-server-sdk\n",
      "Version: 9.4.0\n",
      "Summary: LaunchDarkly SDK for Python\n",
      "Home-page: https://docs.launchdarkly.com/sdk/server-side/python\n",
      "Author: LaunchDarkly\n",
      "Author-email: dev@launchdarkly.com\n",
      "License: Apache-2.0\n",
      "Location: /opt/conda/lib/python3.10/site-packages\n",
      "Requires: certifi, expiringdict, launchdarkly-eventsource, pyRFC3339, semver, urllib3\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show langchain launchdarkly-server-sdk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e16a421-c1ce-4be9-9cd9-6e6623e5f7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load LaunchDarkly SDK from .env.\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29906603-1f5f-4ed0-b314-8a3ce4ca74ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "LD_TEST_SDK = os.environ.get(\"LD-TEST-SDK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadcf099-217b-45d8-bcd1-83e73b404e3b",
   "metadata": {},
   "source": [
    "## Setting up Amazon Bedrock with LangChain ü¶úÔ∏èüîó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "570d0a38-04bc-4a25-b2b4-f3d15aefc2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_aws import ChatBedrock\n",
    "\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name=\"bedrock-runtime\",\n",
    "    region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "model_id = \"anthropic.claude-3-haiku-20240307-v1:0\"\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4629ed-9062-4689-801a-4b6ec559edf5",
   "metadata": {},
   "source": [
    "## Writing a LangChain chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d48b2117-7208-4466-8d05-8303cacdb17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "helpful_prompt_template = PromptTemplate.from_template(\"\"\"\\\n",
    "You are a helpful assistant. Please answer the user's question in a concise\n",
    "and easy to understand manner.\n",
    "\n",
    "Here is a question: {question}\"\"\")\n",
    "\n",
    "llm = ChatBedrock(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "helpful_chain = helpful_prompt_template | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eebca857-bbfb-4dfd-a68a-46d9a4670883",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How can you use feature management to safely and quickly deploy software?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5db5f3dd-a69a-463c-aab3-c1a445f90d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature management allows you to safely and quickly deploy software by:\n",
      "\n",
      "1. Separating feature releases from code deployments: This allows you to test and validate new features independently without impacting the entire application.\n",
      "\n",
      "2. Enabling feature flags/toggles: Feature flags let you turn features on/off remotely, making it easy to enable or disable functionality as needed.\n",
      "\n",
      "3. Gradual rollouts: You can gradually roll out new features to a subset of users, monitor performance, and expand the rollout incrementally.\n",
      "\n",
      "4. A/B testing: Feature management supports A/B testing, allowing you to compare different versions of a feature and determine the optimal configuration.\n",
      "\n",
      "5. Reducing risk: By decoupling feature releases from deployments, you can reduce the risk of breaking changes and quickly roll back problematic features.\n",
      "\n",
      "In summary, feature management provides the flexibility and control to deploy software safely and quickly by separating concerns, enabling remote feature toggles, and supporting gradual rollouts and A/B testing."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in helpful_chain.stream({\"question\": question}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4acd13e0-f8e3-40c9-931f-aa3112860292",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's an interesting question about feature management and software deployment. Could you tell me a bit more about your specific situation and what you're hoping to achieve with feature management? What are some of the challenges or concerns you've encountered in this area that you're hoping to address?\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eliza_prompt_template = PromptTemplate.from_template(\"\"\"\\\n",
    "You are a Rogerian therapist. Please answer the user's question with a question\n",
    "on the same topic. If you don't understand the user's question, please ask them\n",
    "to tell you more.\n",
    "\n",
    "Here is a question: {question}\"\"\")\n",
    "chain = eliza_prompt_template | llm | output_parser\n",
    "chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c196dd-0170-439b-b9da-c5f9a53681ac",
   "metadata": {},
   "source": [
    "## Setting up the LaunchDarkly SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa3bae6b-d2ff-43a4-bbc0-5a57205743d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ldclient\n",
    "from ldclient.config import Config\n",
    "\n",
    "# Test Environment\n",
    "ldclient.set_config(Config(LD_TEST_SDK))\n",
    "client = ldclient.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98532779-dad3-447b-9f79-7d6003661d22",
   "metadata": {},
   "source": [
    "## Selecting a prompt with a feature flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "250cf5cf-ea7e-4f98-b6bc-40c5fb060323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import ConfigurableField\n",
    "\n",
    "prompt = helpful_prompt_template.configurable_alternatives(\n",
    "        ConfigurableField(id=\"prompt\"),\n",
    "        default_key=\"helpful\",\n",
    "        eliza=eliza_prompt_template\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87996e85-217f-49cf-b732-24bf1a8a27bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's an interesting question about feature management and software deployment. Could you tell me a bit more about your specific situation and what you're hoping to achieve with feature management? What are some of the challenges or concerns you've encountered in this area that you're hoping to address?"
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in (chain.with_config(configurable={\"prompt\": \"eliza\"}).stream({\"question\": question})):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1487edaa-62bf-4120-9985-dff25a06aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldclient import Context\n",
    "\n",
    "# Test Environment\n",
    "context = Context.builder(LD_TEST_SDK).name(\"David\").build()\n",
    "\n",
    "prompt_variation = client.variation(\"langchain-prompts\", context, \"helpful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e662617-46e0-4e19-bbe6-ceed948a1c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helpful'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "160c1a85-73d2-48ea-9cd4-18ccec3620ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature management allows you to safely and quickly deploy software by:\n",
      "\n",
      "1. Separating feature deployment from code deployment: This allows you to release new features gradually and independently of the underlying codebase.\n",
      "\n",
      "2. Enabling feature flags: Feature flags let you turn features on/off remotely without redeploying the entire application.\n",
      "\n",
      "3. Controlling feature rollout: You can gradually roll out new features to a subset of users, monitor their performance, and expand the rollout as needed.\n",
      "\n",
      "4. Providing a kill switch: If an issue is detected, you can quickly disable a problematic feature without impacting the entire application.\n",
      "\n",
      "5. Gathering user feedback: Feature management allows you to collect data on feature usage and user behavior, informing future development decisions.\n",
      "\n",
      "In summary, feature management gives you more control, flexibility, and visibility when deploying new software, leading to safer and faster releases."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in (chain.with_config(configurable={\"prompt\": prompt_variation}).stream({\"question\": question})):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77de486f-6201-43bd-869a-276f4e2d84e8",
   "metadata": {},
   "source": [
    "# Adding a new variation to a feature flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5adff28-d863-4700-a7cf-238ee270df9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import ConfigurableField\n",
    "\n",
    "expert_prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an expert assistant. Reply to the the user's question with a detailed and technical answer.\n",
    "\n",
    "Here is a question: {question}\"\"\")\n",
    "\n",
    "prompt = helpful_prompt_template.configurable_alternatives(\n",
    "        ConfigurableField(id=\"prompt\"),\n",
    "        default_key=\"helpful\",\n",
    "        eliza=eliza_prompt_template,\n",
    "        # --- New alternative\n",
    "        expert=expert_prompt_template\n",
    "        # ---\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55583272-a8d4-49e7-918a-c3ffaf03f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ldclient import Context\n",
    "\n",
    "context = Context.builder(\"sdk-a23ff921-ce1e-4066-90a9-b7b7d4945a34\").name(\"David\").build()\n",
    "prompt_variation = client.variation(\"langchain-prompts\", context, \"helpful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "820eb165-40a5-4a58-98d6-5b345e347b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'helpful'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c83a133-d19a-4dd6-b18f-b3d546be9522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature management allows you to safely and quickly deploy software by:\n",
      "\n",
      "1. Enabling Incremental Rollouts: You can gradually roll out new features to a subset of users, monitor their performance, and then expand the rollout.\n",
      "\n",
      "2. Providing Feature Flags: Feature flags allow you to enable or disable specific features remotely, without the need for a full code deployment.\n",
      "\n",
      "3. Reducing Risk: By using feature management, you can reduce the risk of deploying new features by isolating them and controlling their release.\n",
      "\n",
      "4. Gathering Feedback: You can gather user feedback on new features before a full release, allowing you to make adjustments based on real-world usage.\n",
      "\n",
      "5. Improving Agility: Feature management enables faster development cycles and more frequent, smaller deployments, improving your overall agility.\n",
      "\n",
      "In summary, feature management helps you deploy software more safely and quickly by allowing you to control feature releases, gather feedback, and reduce the risk of deployments."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in (chain.with_config(configurable={\"prompt\": prompt_variation}).stream({\"question\": question})):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb84dda-4bf8-4fce-91b7-f4ea5c6ff513",
   "metadata": {},
   "source": [
    "# Storing prompts in feature flags variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524af126-b30a-4a8d-9ab9-4083e9907726",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = helpful_prompt_template.configurable_fields(\n",
    "    template=ConfigurableField(\n",
    "        id=\"prompt_template\",\n",
    "    )\n",
    ")\n",
    "\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "variation = client.variation(\"langchain-prompt-templates\", context, prompt.default.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ac5bc40-061d-4e6d-8822-de7374a2616e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'template': \"You are a Rogerian therapist. Please answer the user's question with a question on the same topic. If you don't understand the user's question, please ask them to tell you more.\\n\\nHere is a question: {question}\"}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70d147aa-43fa-4ce4-a4ea-9a0262c8d0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That's an interesting question about feature management and software deployment. Could you tell me a bit more about your specific goals or challenges in this area? What are you hoping to achieve with feature management, and what concerns do you have around safely and quickly deploying your software? I'd be happy to explore this topic further with you, but I want to make sure I understand your perspective and needs first."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in (chain.with_config(configurable={\"prompt_template\": variation.get(\"template\")}).stream({\"question\": question})):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043f99c2-7067-42bb-ab3e-69fbdb8782cd",
   "metadata": {},
   "source": [
    "# Storing prompt template variables in feature flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3f928066-868a-4c8f-99a2-f848775a426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate.from_template(\"\"\"\\\n",
    "You are an {adjective} assistant. Answer the user's question {instruction}. Question: {question}\"\"\")\n",
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "63fb0503-2e19-49df-85a6-4ab6b8fbb0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation = client.variation(\"langchain-template-variables\", context,\n",
    "   {\"adjective\": \"helpful\", \"instruction\": \"in a concise and easy to understand manner.\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "943d02ba-0123-4dbc-ae88-1d8f67737f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'adjective': 'sympathetic',\n",
       " 'instruction': 'by asking them to tell you more about their question.'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb005d0c-eaa9-4a2d-8d15-7b7d1b786d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with that! Could you tell me a bit more about the context of your question? What kind of software are you looking to deploy, and what are your main goals or concerns when it comes to the deployment process? The more details you can provide, the better I can tailor my response to your specific needs."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in chain.stream({\"question\": question, **variation}):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8723b-67df-4f14-add2-d6022d459fb7",
   "metadata": {},
   "source": [
    "# Selecting and configuring LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "13ac4f78-72d8-433c-b6e3-d7bd2fc4909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import SystemMessage\n",
    "\n",
    "llm = llm.configurable_alternatives(\n",
    "    ConfigurableField(id=\"model\"),\n",
    "    default_key=\"claude\",\n",
    "    titan=ChatBedrock(model_id=\"anthropic.titan-instant-v1\"),\n",
    "    command=ChatBedrock(model_id=\"anthropic.titan-instant-v1\"),\n",
    "    mistral=ChatBedrock(model_id=\"mistral.mistral-7b-instruct-v0:2\"),\n",
    ")\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"You are a helpful assistant. Please answer the user's question in a concise and easy to understand manner.\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chat_chain = chat_prompt_template | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8067a914-e534-45fd-9546-4fd81fe0e433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mistral'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation = client.variation(\"langchain-llm-models\", context, \"claude\")\n",
    "variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dad5591f-1979-4ce4-a2c4-dcde541216ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature management is a practice that allows you to control the release of new features or changes to your software in a safe and controlled manner. Here's how you can use it to deploy software quickly and safely:\n",
      "\n",
      "1. **Isolate new features:** Use feature flags to isolate new features or changes from the rest of the codebase. This allows you to test the new feature in a controlled environment without affecting the entire user base.\n",
      "\n",
      "2. **Gradually roll out features:** Use a phased rollout strategy to gradually release new features to a small percentage of users. This helps you identify and fix any issues before releasing the feature to a larger audience.\n",
      "\n",
      "3. **Monitor feature performance:** Use analytics and monitoring tools to track the performance of new features in real-time. This allows you to quickly identify any issues and make adjustments as needed.\n",
      "\n",
      "4. **Roll back features:** If a new feature causes issues or negatively impacts user experience, you can use feature flags to roll back the feature and revert to the previous version. This helps minimize the impact of the issue on your users.\n",
      "\n",
      "5. **Collaborate with stakeholders:** Communicate regularly with stakeholders, such as product managers, developers, and users, to gather feedback and make informed decisions about when and how to release new features. This helps ensure that everyone is aligned on the goals and objectives of the release."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in (chat_chain.with_config(configurable={\"model\": variation}).stream({\"question\": question})):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98fd11-c0b0-4a19-94dc-8cd680b2a598",
   "metadata": {},
   "source": [
    "### Configure the parameters of a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e2dd329-e296-43a2-89fd-07a5dc6d8391",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatBedrock(model_id=\"anthropic.claude-3-haiku-20240307-v1:0\").configurable_fields(\n",
    "    model_id=ConfigurableField(id=\"model_id\"),\n",
    "    model_kwargs=ConfigurableField(id=\"model_kwargs\")\n",
    ")\n",
    "chain = helpful_prompt_template | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b45ac68f-55f5-4646-81ef-057a083e4948",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation = client.variation(\"langchain-llm-model-kwargs\", context,\n",
    "    {\"model_id\": \"anthropic.claude-3-haiku-20240307-v1:0\", \"model_kwargs\": {\"temperature\": 0.5}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66b9f4e1-b283-4b3e-b0d2-760b1024e54b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_id': 'anthropic.claude-3-sonnet-20240229-v1:0'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1704ac38-ff20-4e82-b47b-97cd39a649bb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature management is a software development practice that allows you to safely and quickly deploy software by controlling the release of new features. Here are some key ways feature management can help:\n",
      "\n",
      "1. Gradual Rollout: Feature management enables you to gradually roll out new features to a subset of users first, rather than deploying to all users at once. This allows you to test the feature, gather feedback, and make adjustments before a full release.\n",
      "\n",
      "2. Feature Flags: Feature flags are toggles that allow you to turn features on or off remotely, without having to redeploy the entire application. This makes it easy to quickly enable, disable, or A/B test new features.\n",
      "\n",
      "3. Targeted Releases: You can use feature management to target the release of new features to specific user segments, regions, or devices. This helps minimize the impact of issues and allows you to test features with a limited audience first.\n",
      "\n",
      "4. Reduced Risk: By controlling the release of new features, feature management helps reduce the risk of bugs or regressions being introduced to your production environment. This leads to more stable and reliable software deployments.\n",
      "\n",
      "5. Faster Iteration: The ability to quickly enable, disable, and test new features allows development teams to iterate more rapidly and get new functionality to users faster.\n",
      "\n",
      "Overall, feature management is a powerful technique that enables software teams to deploy new features safely, quickly, and with more control. This helps improve the user experience and accelerate the software development lifecycle."
     ]
    }
   ],
   "source": [
    "# Stream\n",
    "for chunk in (chat_chain.with_config(configurable={**variation}).stream({\"question\": question})):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b95fcf6-facc-48da-8577-9f7be4c02f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
